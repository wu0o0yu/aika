<!DOCTYPE HTML>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Blog - Aika</title>
	<link rel="stylesheet" href="css/style.css" type="text/css">
	<link rel="shortcut icon" href="images/favicon.png" />

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
					(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-77552310-1', 'auto');
		ga('send', 'pageview');

	</script>
</head>
<body>
<div id="header">
	<div>
		<div class="logo">
			<a rel="canonical" href="https://aika.network"></a>
		</div>
		<ul id="navigation">
			<li>
				<a rel="canonical" href="https://aika.network">Overall idea</a>
			</li>
			<li class="active">
				<a rel="canonical" href="blog.html">Blog</a>
			</li>
			<li>
				<a rel="canonical" href="inference.html">Inference</a>
			</li>
			<li>
				<a rel="canonical" href="training.html">Training</a>
			</li>
			<li>
				<a rel="canonical" href="usage.html">Examples</a>
			</li>
			<li>
				<a rel="canonical" href="resources.html">Resources</a>
			</li>
			<li>
				<a rel="canonical" href="https://github.com/aika-algorithm/aika">GitHub</a>
			</li>
		</ul>
	</div>
</div>
	<div id="contents">
		<div align="right"><span style='color: #FF0000;'>October 09, 2020</span></div>
		<div class="features">
			<h1>Blog</h1>

			<h2>1. January 2020</h2>
			<b><a href="https://towardsdatascience.com/can-the-dynamic-linking-of-neural-activations-bring-us-closer-to-strong-ai-bc85b64c9f82">Can the dynamic linking of neural activations bring us closer to strong AI?</a></b><br/>
			<br/>
			<b>Most artificial neural networks ignore the spiking nature of biological neural networks to simplify the underlying model and enable learning techniques such as back-propagation. But by doing so, aren’t we possibly rejecting one of the most central principles of biological neural networks?
			</b>
			<br/>
			Within most artificial neural network models, an activation is just a real numbered value associated with the neuron itself. But that’s not what is happening within a biological neural network. Here, an activation occurs when the threshold of a neuron is exceeded and there is an exact point in time associated with it. A prerequisite for this event to occur is that several other input activations have been fired before the current activation.
			...
			<br/><br/>

			<h2>23. September 2019</h2>
			<b><a href="https://towardsdatascience.com/using-information-gain-for-the-unsupervised-training-of-excitatory-neurons-e069eb90245b">Using Information Gain for the Unsupervised Training of Excitatory Neurons</a></b><br/>
			<br/>
			<b>Looking for a biologically more plausible way to train a neural network.</b>
			<br/>
			Traditionally, artificial neural networks have been trained using the Delta rule and backpropagation. But this contradicts the findings that the neurosciences have made on the function of the brain. There simply is no gradient error signal that is propagated backwards through biological neurons.
			...
			<br/><br/>
			<h2>28. August 2019</h2>
			<b><a href="https://towardsdatascience.com/using-meta-neurons-to-learn-facts-from-a-single-training-example-781ca0b7424d">Using Meta-Neurons to learn facts from a single training example</a></b><br/>
			<br/>
			<b>When people see a new animal, meet a new person or visit a new place, they don’t need to repeat that experience thousands of times to remember it — so why should computers have to?</b><br/>
			<br/>
			Human learning comes in two forms, a fast and a slow one. The slow one requires a lot of repetition which seems to be necessary to conquer a new cognitive field such as learning a new language. But once a field is mastered, learning new facts within this field requires very few, possibly even only one example. It appears, that the brain regions involved in processing this field have been pre wired to the regions they depend on. So once a new fact needs to be learned, this pre wiring is used to speed up the training of the neurons involved in processing this new fact. It is important, that during this training a distinction is made between new and existing knowledge. Otherwise, all connections based on already existing knowledge would be lost due to the repeated learning of something that is already known. So the neurons involved in the processing of an already known fact must therefore be able to suppress the formation of new knowledge.
			...
			<br/><br/>
			<h2>9. August 2019</h2>
			<b><a href="https://medium.com/@lukasmolzberger/how-to-efficiently-propagate-activations-in-a-massive-neural-network-3faed60cb8f5">How to efficiently propagate activations in a massive neural network</a></b><br/>
			<br/>
			<b>When people see a new animal, meet a new person or visit a new place, they don’t need to repeat that experience thousands of times to remember it — so why should computers have to?</b><br/>
			<br/>
			In traditional neural networks using the sigmoid activation function, all neurons are more or less activated. There is no clear case of an inactive neuron here.
			...
			<br/><br/>
			<h2>3. August 2019</h2>
			<b><a href="https://towardsdatascience.com/on-adding-negative-recurrent-synapses-to-a-neural-network-25a28409a6f2">On adding negative feedback synapses to a neural network</a></b><br/>
			<br/>
			<b>The missing link in deep neural networks</b><br/>
			<br/>
			The special thing about adding negative recurrent synapses to a neural network is that they introduce inner states within the network.
			...
			<br/><br/>
			<h2>9. December 2018</h2>
			<b><a href="https://towardsdatascience.com/on-integrating-symbolic-inference-into-deep-neural-networks-22ed13ebbba9">On integrating symbolic inference into deep neural networks</a></b><br/>
			<br/>
			<b>What makes biological neural networks so superior to their technical counterparts? Is there anything we have overlooked so far?</b><br/>
			<br/>
			Deep neural networks have been a tremendous success story over the last couple of years. Many advances in the field of AI, such as recognizing real world objects, fluently translating natural language or playing GO ...
			<br/><br/>
			<h2>25. October 2017</h2>
			<b><a href="https://data-science-blog.com/blog/2017/10/25/aika-ein-semantisches-neuronales-netzwerk/">Aika: Ein semantisches neuronales Netzwerk</a></b><br/>
			<br/>
			Wenn es darum geht Informationen aus natürlichsprachigen Texten zu extrahieren, stehen einem verschiedene Möglichkeiten zur Verfügung. Eine der ältesten und wohl auch am häufigsten genutzten Möglichkeiten ist die der regulären Ausdrücke. Hier werden exakte Muster definiert und in einem Textstring gematcht. Probleme bereiten diese allerdings, wenn kompliziertere semantische Muster gefunden werden sollen oder wenn verschiedene Muster aufeinander aufbauen oder miteinander interagieren sollen. ...
			<br/><br/>
			<h2>20. June 2017</h2>
			<a href="https://jaxenter.de/texte-verstehen-mit-machine-learning-58487">Machine-Learning-Bibliothek hilft dabei Texte zu verstehen</a><br/>
			<br/>
			Volltextsuche ist ein Kernbestandteil des Internetzeitalters, nichtsdestotrotz lässt sie bis heute viel zu Wünschen übrig. Sie ist ganz hervorragend dazu geeignet, um exakte Worttreffer in einer großen Menge an Dokumenten zu finden. Was allerdings bis heute noch nicht zuverlässig funktioniert ist, ein Wort nur in seiner gewünschten Bedeutung zu finden. Genau hier kann die Java-Bibliothek Aika weiterhelfen. ...
			<br/><br/>
		</div>
	</div>
</body>
</html>