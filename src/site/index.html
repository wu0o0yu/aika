<!DOCTYPE HTML>
<html>
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<title>Aika: An Artificial Intelligence for Knowledge Acquisition</title>
	<link rel="stylesheet" href="css/style.css" type="text/css">
	<link rel="shortcut icon" href="images/favicon.png" />

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
					(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-77552310-1', 'auto');
		ga('send', 'pageview');

	</script>
</head>
<body>
	<div id="header">
		<div>
			<div class="logo">
				<a rel="canonical" href="https://aika.network"></a>
			</div>
			<ul id="navigation">
				<li class="active">
					<a rel="canonical" href="https://aika.network">Overall idea</a>
				</li>
				<li>
					<a rel="canonical" href="blog.html">Blog</a>
				</li>
				<li>
					<a rel="canonical" href="inference.html">Inference</a>
				</li>
				<li>
					<a rel="canonical" href="nlp.html">NLP</a>
				</li>
				<li>
					<a rel="canonical" href="training.html">Training</a>
				</li>
				<li>
					<a rel="canonical" href="usage.html">Examples</a>
				</li>
				<li>
					<a rel="canonical" href="resources.html">Resources</a>
				</li>
				<li>
					<a rel="canonical" href="https://github.com/aika-algorithm/aika">GitHub</a>
				</li>
			</ul>
		</div>
	</div>
	<div id="contents">
		<div align="right"><span style='color: #FF0000;'>February 07, 2022</span></div>
		<div id="features">
			<h1>About the AIKA Neural Network</h1>
			<div>
				<p>
					<b>
						Aika (<u>A</u>rtificial <u>I</u>ntelligence for <u>K</u>nowledge <u>A</u>cquisition) is a new
						type of artificial neural network designed to more closely mimic the behavior of a biological
						brain and to bridge the gap to classical AI. A key design decision in the AIKA network is to
						conceptually separate the activations from their neurons, meaning that there are two separate
						graphs. One graph consisting of neurons and synapses representing the knowledge the network has
						already acquired and another graph consisting of activations and links describing the information
						the network was able to infer about a concrete input data set. There is a one-to-many relation
						between the neurons and the activations. For example, there might be a neuron representing a
						word or a specific meaning of a word, but there might be several activations of this neuron,
						each representing an occurrence of this word within the input data set. A consequence of this
						decision is that we must give up on the idea of a fixed layered topology for the network, since
						the sequence in which the activations are fired depends on the input data set. Within the
						activation network, each activation is grounded within the input data set, even if there are
						several activations in between. This means links between activations server multiple purposes:
					</b>
				</p>
				<p>

				<ul>
					<li>They propagate the activation value.</li>
					<li>They propagate the binding-signal, that is used for the linking process.</li>
					<li>They establish an approximate causal relation through the fired timestamps of their input and
						output activations.
					</li>
					<li>They allow the training gradient to be propagated backwards.</li>
					<li>Negative feedback links create mutually exclusive branches within the activations network.</li>
					<li>Positive feedback links allow the binding neurons of a pattern neuron ensemble to support each
						other, by feeding the activation value of the patten neuron back to its input binding-neurons.
					</li>
				</ul>
				</b>
				</p>
				<p>

						The AIKA network uses four different types of neurons:
				<ul>
					<li>Pattern-Neurons (PN)</li>
					<li>Binding-Neurons (BN)</li>
					<li>Inhibitory-Neurons (IN)</li>
					<li>Category-Neurons (CN)</li>
				</ul>

				</b>
				</p>
				<p>
					The Pattern-Neurons and the Binding-Neurons are both conjunctive in nature while the Inhibitory-Neurons
					and the Category-Neurons are disjunctive. The Binding-Neurons are kind of the glue code of the whole
					network. On the one hand, they bind the input-features of a pattern to the pattern-neuron and on the
					other hand receive negative feedback synapses from the inhibitory neurons which allow them to either
					be suppressed by an opposing pattern or allow themselves suppress another conflicting pattern. Similar
					to the neuron types there are also several different types of synapses, depending on wich types of
					neurons they connect. For example, the input synapses of an inhibitory neuron are always linked to
					Binding-Neurons, while the input synapses of Category-Neurons are always linked to pattern-neurons.
				</p>
				<p>
					The following types of synapses exist within the AIKA network:
				<ul>
					<li>PrimaryInputBNSynapse ((PN|CN) -> BN)</li>
					<li>RelatedInputBNSynapse (BN -> BN)</li>
					<li>SamePatternBNSynapse (BN -> BN)</li>
					<li>PositiveFeedbackSynapse (PN -> BN)</li>
					<li>NegativeFeedbackSynapse (IN -> BN)</li>
					<li>PatternSynapse (BN -> PN)</li>
					<li>CategorySynapse (PN -> CN)</li>
					<li>InhibitorySynapse (BN -> IN)</li>
				</ul>
				</p>
				<p>
					Depending on their source activation two types of binding-signals can be distinguished: The
					pattern-binding-signal and the branch-binding-signal. The pattern-binding-signal originates at a
					pattern-activation and is used to bind the input-features of a pattern to the pattern itself.
					The branch-binding-signal originates at a binding-activation and is used to distinguish the mutually
					exclusive branches from each other. The pattern-binding-signal is also carrying a scope that allows
					it to distinguish between different pattern binding ensembles. The scope of the binding-signal
					changes when being propagated through certain types of synapses such as the PrimaryInputBNSynapse
					of the RelatedInputBNSynapse. The scope is used during the linking process to verify the validity
					of creating a certain new link.
				</p>
				<p>
					As already mentioned, the Binding-Neurons of a pattern neuron ensemble are used to bind this pattern
					to its input features. To verify that all the input-features occurred in the correct relation to
					each other the SamePatternBNSynapse is used. The SamePatternBNSynapse connects two Binding-Neurons
					within the same pattern neuron ensemble and is only linked both ends of the synapse have been reached
					by the same binding-signal. Therefore, the SamePatternBNSynapse is used to avoid what
					is called the superposition catastrophe.
				</p>
				<p>
					Since the Category-Neuron passes on the pattern-binding-signal of its input Pattern-Neuron, it can
					act as a category slot, therefore allowing the network great flexibility in abstracting concepts.
				</p>
				<p>
					Initially, the network starts out empty and is then gradually populated during training. The
					induction of new neurons and synapses is guided by a network of template neurons and synapses.
				</p>
				<p>
					Since this type of network contains cycles, the usual backpropagation algorithm will not work very
					well here. Also, relying on handcrafted labels that are applied to the output of the network can be
					highly error-prone and can create a large distance between our training signal and the weights that
					we would like to adjust. This is the reason for the huge number of training examples required for
					classical neural networks. Hence, we would like to train the network more locally from the patterns
					that occur in the input data without relying on supervised training labels. This is where Shannon
					entropy comes in quite handy. Take, for example, a word whose input features are its individual
					letters. In this example, we can measure the amount of information of each letter by calculating
					the Shannon entropy. Then we can look at the word pattern neuron as a way of compressing the
					information given by the individual letter neurons. The word neuron requires a lot less information
					to communicate the same message as the sum of the individual letters. This compression, or
					information gain, can be formalized using the mutual information, which can then be used to derive
					an objective function for our training algorithm.
				</p>
				<p>
					A consequence of using entropy as a source for the training signal is that we need to know what the
					underlying probability distribution is for each neuron and for each synapse. That is, we need to
					count how often each neuron is fired. But determining this statistic involves some challenges as
					well:

				<ul>
				<li>
					Not all the neurons exist right from the start. Therefore, we need to keep track of an offset for
					each neuron, so that we can compute how many training examples this neuron has already seen.
				</li>
				<li>
					Not all activations cover the same space within the input data set. For example there are a lot more
					letters than there are words in a given text. This has to be taken into account
					when determining the event space for the probability.
				</li>
				<li>
					Then there is the problem that right after a new neuron is introduced, there are very few training
					instances for this statistic, yet we still want to adjust the weights of this neuron.
					Hence we need a conservative estimate of what we already know for certain about the probabilities.
					Since entropy is based on the surprisal (-log(p)), which gets large when the probability
					gets close to zero, we can use the cumulative beta distribution to determine what the maximum
					probability is that can be explained by the observed frequencies.
				</li>
				<li>
					Another problem is related to concept drift. As we are using the distribution to adjust the synapse
					weights, this leads to changes in the activation patterns and therefore to changes
					in the distributions again.
				</li>
			</ul>
				</p>
			</div>
		</div>
	</div>
</body>
</html>